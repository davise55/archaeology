{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aphrodisias analysis notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "import pygeoprocessing\n",
    "from osgeo import gdal, ogr\n",
    "import statistics\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import wilcoxon, pointbiserialr\n",
    "import seaborn\n",
    "\n",
    "\n",
    "gdal.UseExceptions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_folder = Path(r\"C:\\Users\\lizad\\OneDrive\\Desktop\\Brown\\Dissertation\\GIS\\Aphrodisias\")\n",
    "\n",
    "target_nodata = -9999.0\n",
    "\n",
    "dem_raster_path = gis_folder / \"processed_data/DEM_clip.tif\"\n",
    "dem_raster_info = pygeoprocessing.get_raster_info(str(dem_raster_path))\n",
    "\n",
    "dem_treeheight_raster_path = gis_folder / \"viewsheds/DEM_treeheight_combined.tif\"\n",
    "treeheight_raster_path = gis_folder / \"viewsheds/treeheight_UTM_aligned.tif\"\n",
    "\n",
    "slope_raster_path = gis_folder / \"slope_35N.tif\"\n",
    "tri_raster_path = gis_folder / \"tri_35N.tif\"\n",
    "landcover_raster_path = gis_folder / \"landcover_35N.tif\"\n",
    "\n",
    "church_vector_path = gis_folder / \"aphrodisias_churches.gpkg\"\n",
    "church_name_field = \"field_8\"\n",
    "city_center_vector_path = gis_folder / \"center_35N.gpkg\"\n",
    "city_outline_vector_path = gis_folder / \"city_outline.gpkg\"\n",
    "\n",
    "church_raster_path = gis_folder / \"churches_35N.tif\"\n",
    "city_center_raster_path = gis_folder / \"center_35N.tif\"\n",
    "\n",
    "multicriteria_lcps_vector_path = gis_folder / \"final_lca/multicriteria_traditional.gpkg\"\n",
    "slope_lcps_vector_path = gis_folder / \"final_lca/slope_traditional.gpkg\"\n",
    "\n",
    "# Intermediate files created within the script\n",
    "## Aligned and clipped rasters\n",
    "dem_raster_clip = gis_folder / \"DEM_UTM_35N_clip.tif\"\n",
    "landcover_raster_clip = gis_folder / \"landcover_35N_clip.tif\"\n",
    "slope_raster_clip = gis_folder / \"slope_35N_clip.tif\"\n",
    "tri_raster_clip = gis_folder / \"tri_35N_clip.tif\"\n",
    "\n",
    "## Landcover reclassified into resistence\n",
    "landcover_reclass = gis_folder / \"landcover_reclass.tif\"\n",
    "\n",
    "## Tobler Hiking rasters\n",
    "tobler_surface_raster_white = gis_folder / \"tobler_surface_white.tif\"\n",
    "tobler_rescale_raster_white = gis_folder / \"tobler_rescale_white.tif\"\n",
    "\n",
    "tobler_surface_raster_mp = gis_folder / \"tobler_surface_mp.tif\"\n",
    "tobler_rescale_raster_mp = gis_folder / \"tobler_rescale_mp.tif\"\n",
    "\n",
    "## Combined cost surface rasters\n",
    "cost_surface_white = gis_folder / \"cost_surface_white.tif\"\n",
    "cost_surface_mp = gis_folder / \"cost_surface_mp.tif\"\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for circuitscape church analysis\n",
    "connectivity_buffer_dist_m = 100\n",
    "buffered_church_vector = gis_folder / f\"churches_buffer_{connectivity_buffer_dist_m}m.gpkg\"\n",
    "\n",
    "circuitscape_church_raster = gis_folder / \"circuitscape_church_source.tif\"\n",
    "circuitscape_city_raster = gis_folder / \"circuitscape_city_ground.tif\"\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for connectivity analysis and statistics\n",
    "connectivity_raster = gis_folder / \"circuitscape/omnidirectional_curmap_250516.tif\"\n",
    "\n",
    "sample_points_folder = gis_folder / \"sample_points_analysis\"\n",
    "sample_area_vector_name  = \"sample_area{}.gpkg\"\n",
    "sample_area_buffer_m = 500\n",
    "sample_point_iterations = 100\n",
    "\n",
    "buffered_church_name = \"churches_buffer_{}m{}.gpkg\"  # replace with connectivity_buffer_dist_m, results_suffix\n",
    "buffered_church_zonal_stat_name = \"churches_buffer_{}m_zonal_stats{}.gpkg\"  # replace with connectivity_buffer_dist_m, results_suffix\n",
    "\n",
    "current_var_structure = \"omnidirectional_curmap_{}\"\n",
    "church_var = \"church\"\n",
    "\n",
    "# TODO\n",
    "church_designation_column = \"church_designation\"\n",
    "church_designation_value = \"suburban\"\n",
    "church_designation_results_suffix = \"suburban\"\n",
    "\n",
    "## Pathnames for saving iteration outputs\n",
    "sample_points_vector_name = \"sample_points{}_{}.gpkg\"  # replace with results_suffix, sample number\n",
    "\n",
    "buffered_sample_points_vector_name = \"buffered_sample_points_{}m{}_{}.gpkg\"  # replace with connectivity_buffer_dist_m, results_suffix, sample number\n",
    "buffered_sample_points_zonal_stats_vector_name = \"zonal_stats_sample_points_{}m_zonal_stats{}_{}.gpkg\"  # replace with connectivity_buffer_dist_m, results_suffix, sample number\n",
    "\n",
    "combined_stats_vector_pathname =  \"connectivity_statistics_sample_points{}_{}.gpkg\"  # replace with results_suffix, sample number\n",
    "histogram_name = \"histogram_sample{}_{}_{}.png\"  # replace with results_suffix, sample number, zonal_stat\n",
    "\n",
    "combined_statistics_csv_name = \"combined_statistical_results{}.csv\"  # replace with results_suffix\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for viewshed analysis\n",
    "church_height_m = 5\n",
    "viewshed_distance_m = 10000\n",
    "\n",
    "viewshed_results_folder = gis_folder / \"viewsheds\"\n",
    "viewshed_results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "composite_viewshed = viewshed_results_folder / f\"composite_viewshed.tif\"\n",
    "\n",
    "viewshed_treeheight_results_folder = gis_folder / \"viewsheds/treeheight\"\n",
    "viewshed_treeheight_results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "composite_treeheight_viewshed = viewshed_treeheight_results_folder / \"composite_treehight_viewshed.tif\"\n",
    "\n",
    "viewshed_sample_area = gis_folder / \"viewsheds/cumulative_viewsheds_bbox.gpkg\"\n",
    "cumulative_viewshed_analysis_folder = gis_folder / \"viewsheds/cumulative_viewshed_analysis\"\n",
    "\n",
    "viewshed_sample_point_iterations = 100\n",
    "viewshed_sample_points_vector_name = cumulative_viewshed_analysis_folder / f\"viewshed_sample_points_{viewshed_sample_point_iterations}.gpkg\"\n",
    "random_composite_viewshed = cumulative_viewshed_analysis_folder / \"random_composite_viewshed.tif\"\n",
    "\n",
    "## Pathnames for saving iteration outputs\n",
    "aligned_file_name = \"aligned_{}\"\n",
    "viewshed_file_name = \"viewshed_{}.tif\"\n",
    "treeheight_viewshed_file_name = \"treehight_viewshed_{}.tif\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterizing Churches and city center to match the DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and burn church locations\n",
    "if not church_raster_path.exists:\n",
    "    pygeoprocessing.new_raster_from_base(\n",
    "        str(dem_raster_path),\n",
    "        str(church_raster_path),\n",
    "        gdal.GDT_Byte,\n",
    "        [0],\n",
    "    )\n",
    "    pygeoprocessing.rasterize(\n",
    "    str(church_vector_path),\n",
    "    str(church_raster_path),\n",
    "    [1],\n",
    "    )\n",
    "\n",
    "# Create and burn city center\n",
    "if not city_center_raster_path.exists:\n",
    "    pygeoprocessing.new_raster_from_base(\n",
    "        str(dem_raster_path),\n",
    "        str(city_center_raster_path),\n",
    "        gdal.GDT_Int16,\n",
    "        [0],\n",
    "    )\n",
    "    pygeoprocessing.rasterize(\n",
    "    str(city_center_vector_path),\n",
    "    str(city_center_raster_path),\n",
    "        [1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate slope and Terrain Ruggedness Index from DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_gdal_ds = gdal.DEMProcessing(str(slope_raster_path), str(dem_raster_path), \"slope\")\n",
    "del slope_gdal_ds\n",
    "\n",
    "# pygeoprocessing.calculate_slope((str(dem_raster_path), 1), str(slope_raster_path))\n",
    "\n",
    "tri_gdal_ds = gdal.DEMProcessing(str(tri_raster_path), str(dem_raster_path), \"tri\")\n",
    "del tri_gdal_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align and clip landscape data to DEM extent\n",
    "dem_raster_info = pygeoprocessing.get_raster_info(str(dem_raster_path))\n",
    "\n",
    "pygeoprocessing.align_and_resize_raster_stack(\n",
    "    [str(dem_raster_path), str(landcover_raster_path), str(slope_raster_path), str(tri_raster_path)],\n",
    "    [str(dem_raster_clip), str(landcover_raster_clip), str(slope_raster_clip), str(tri_raster_clip)],\n",
    "    [\"near\", \"near\", \"near\", \"near\"],\n",
    "    dem_raster_info[\"pixel_size\"],\n",
    "    dem_raster_info[\"bounding_box\"],\n",
    "    raster_align_index=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reclassify landcover into cost surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reclass_df = pd.read_csv(\"./data/esa_worldcover_classification.csv\")\n",
    "cost_value = \"cost_value\"\n",
    "\n",
    "reclass_dict = reclass_df.set_index(\"lucode\").to_dict()[cost_value]\n",
    "\n",
    "pygeoprocessing.reclassify_raster(\n",
    "    (str(landcover_raster_clip),1),\n",
    "    reclass_dict,\n",
    "    str(landcover_reclass),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Tobler's hiking function (Tobler 1993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tobler_surface_raster = gis_folder / \"tobler_surface.tif\"\n",
    "\n",
    "# slope_raster_info = pygeoprocessing.get_raster_info(str(slope_raster_clip))\n",
    "# slope_cell_resolution = statistics.mean([abs(x) for x in slope_raster_info[\"pixel_size\"]])\n",
    "# slope_nodata = slope_raster_info[\"nodata\"][0]\n",
    "\n",
    "# # def tobler_op(slope_array):\n",
    "# #     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array*math.pi/180)+0.05)))\n",
    "# #     return result\n",
    "# # def tobler_op(slope):\n",
    "# #     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(slope+0.05)))\n",
    "# #     return result\n",
    "\n",
    "# def tobler_op(slope_array):\n",
    "#     # Make an array of the same shape full of nodata\n",
    "#     output = np.full(slope_array.shape, slope_nodata)\n",
    "\n",
    "#     # Make a masking array to ignore all nodata areas in the original data\n",
    "#     valid_mask = np.full(slope_array.shape, True)\n",
    "#     valid_mask &= ~pygeoprocessing.array_equals_nodata(slope_array, slope_nodata)\n",
    "\n",
    "#     output[valid_mask] = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array[valid_mask])+0.05)))\n",
    "#     output[valid_mask] = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array[valid_mask]*math.pi/180)+0.05)))\n",
    "\n",
    "#     return output\n",
    "\n",
    "# pygeoprocessing.raster_calculator(\n",
    "#     [(str(slope_raster_clip),1)], \n",
    "#     tobler_op,\n",
    "#     str(tobler_surface_raster),\n",
    "#     gdal.GDT_Float32,\n",
    "#     target_nodata,\n",
    "#     calc_raster_stats=True\n",
    "# )\n",
    "\n",
    "\n",
    "# # Define Tobler rescaling function\n",
    "# def tobler_rescale_op(tobler_surface_array, upper_limit=0.1666667, lower_limit=0.0):\n",
    "#     # Make an array of the same shape full of nodata\n",
    "#     output = np.full(tobler_surface_array.shape, target_nodata)\n",
    "\n",
    "#     # Make a masking array to ignore all nodata areas in the original data\n",
    "#     valid_mask = np.full(tobler_surface_array.shape, True)\n",
    "#     valid_mask &= ~pygeoprocessing.array_equals_nodata(tobler_surface_array, target_nodata)\n",
    "\n",
    "#     # Calculate initial rescaling\n",
    "#     output[valid_mask] = (tobler_surface_array[valid_mask] - lower_limit)/(upper_limit - lower_limit)\n",
    "\n",
    "#     # Force values larger than the upper_limit to equal one\n",
    "#     upper_limit_mask = (tobler_surface_array >= upper_limit) & valid_mask\n",
    "#     output[upper_limit_mask] = 1\n",
    "\n",
    "#     # Force values smaller than the lower_limit to equal zero\n",
    "#     lower_limit_mask = (tobler_surface_array <= lower_limit) & valid_mask\n",
    "#     output[lower_limit_mask] = 0\n",
    "\n",
    "#     return output\n",
    "\n",
    "# tobler_rescale_raster = gis_folder / \"tobler_rescale.tif\"\n",
    "\n",
    "# # Rescaling Tobler's original function\n",
    "# if tobler_rescale_raster.exists():\n",
    "#     tobler_rescale_raster.unlink()\n",
    "# pygeoprocessing.raster_calculator(\n",
    "#     [(str(tobler_surface_raster),1)], \n",
    "#     tobler_rescale_op,\n",
    "#     str(tobler_rescale_raster),\n",
    "#     gdal.GDT_Float32,\n",
    "#     target_nodata,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate modified Tobler's hiking function (White 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_raster_info = pygeoprocessing.get_raster_info(str(slope_raster_clip))\n",
    "slope_cell_resolution = statistics.mean([abs(x) for x in slope_raster_info[\"pixel_size\"]])\n",
    "slope_nodata = slope_raster_info[\"nodata\"][0]\n",
    "\n",
    "# def tobler_op(slope_array):\n",
    "#     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array*math.pi/180)+0.05)))\n",
    "#     return result\n",
    "# def tobler_op(slope):\n",
    "#     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(slope+0.05)))\n",
    "#     return result\n",
    "\n",
    "def tobler_op_white(slope_array):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(slope_array.shape, slope_nodata)\n",
    "\n",
    "    # Make a masking array to ignore all nodata areas in the original data\n",
    "    valid_mask = np.full(slope_array.shape, True)\n",
    "    valid_mask &= ~pygeoprocessing.array_equals_nodata(slope_array, slope_nodata)\n",
    "\n",
    "    output[valid_mask] = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array[valid_mask]*math.pi/180)+0.05)))\n",
    "\n",
    "    return output\n",
    "\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(slope_raster_clip),1)], \n",
    "    tobler_op_white,\n",
    "    str(tobler_surface_raster_white),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    "    calc_raster_stats=True\n",
    ")\n",
    "\n",
    "# Define Tobler rescaling function to convert to 0-1 index\n",
    "def tobler_rescale_op(tobler_surface_array, upper_limit=0.1666667, lower_limit=0.0):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(tobler_surface_array.shape, target_nodata)\n",
    "\n",
    "    # Make a masking array to ignore all nodata areas in the original data\n",
    "    valid_mask = np.full(tobler_surface_array.shape, True)\n",
    "    valid_mask &= ~pygeoprocessing.array_equals_nodata(tobler_surface_array, target_nodata)\n",
    "\n",
    "    # Calculate initial rescaling\n",
    "    output[valid_mask] = (tobler_surface_array[valid_mask] - lower_limit)/(upper_limit - lower_limit)\n",
    "\n",
    "    # Force values larger than the upper_limit to equal one\n",
    "    upper_limit_mask = (tobler_surface_array >= upper_limit) & valid_mask\n",
    "    output[upper_limit_mask] = 1\n",
    "\n",
    "    # Force values smaller than the lower_limit to equal zero\n",
    "    lower_limit_mask = (tobler_surface_array <= lower_limit) & valid_mask\n",
    "    output[lower_limit_mask] = 0\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Rescaling Tobler's original function\n",
    "if tobler_rescale_raster_white.exists():\n",
    "    tobler_rescale_raster_white.unlink()\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(tobler_surface_raster_white),1)], \n",
    "    tobler_rescale_op,\n",
    "    str(tobler_rescale_raster_white),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate modified Tobler's hiking function (Marquez-Perez 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_raster_info = pygeoprocessing.get_raster_info(str(slope_raster_clip))\n",
    "slope_cell_resolution = statistics.mean([abs(x) for x in slope_raster_info[\"pixel_size\"]])\n",
    "slope_nodata = slope_raster_info[\"nodata\"][0]\n",
    "\n",
    "# def tobler_op(slope_array):\n",
    "#     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(np.tan(slope_array*math.pi/180)+0.05)))\n",
    "#     return result\n",
    "# def tobler_op(slope):\n",
    "#     result = (slope_cell_resolution/1000)/(6*np.exp(-3.5*np.abs(slope+0.05)))\n",
    "#     return result\n",
    "\n",
    "def tobler_op_mp(slope_array):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(slope_array.shape, slope_nodata)\n",
    "\n",
    "    # Make a masking array to ignore all nodata areas in the original data\n",
    "    valid_mask = np.full(slope_array.shape, True)\n",
    "    valid_mask &= ~pygeoprocessing.array_equals_nodata(slope_array, slope_nodata)\n",
    "\n",
    "    output[valid_mask] = (slope_cell_resolution/1000)/(4.8*np.exp(-5.3*np.abs((np.tan(slope_array[valid_mask]*math.pi/180)*0.7)+0.03)))\n",
    "\n",
    "    return output\n",
    "\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(slope_raster_clip),1)], \n",
    "    tobler_op_mp,\n",
    "    str(tobler_surface_raster_mp),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    "    calc_raster_stats=True\n",
    ")\n",
    "\n",
    "\n",
    "# Rescaling Tobler's original function\n",
    "if tobler_rescale_raster_mp.exists():\n",
    "    tobler_rescale_raster_mp.unlink()\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(tobler_surface_raster_mp),1)], \n",
    "    tobler_rescale_op,\n",
    "    str(tobler_rescale_raster_mp),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multicriteria cost surface (White 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodata values for toblers and lulc\n",
    "landcover_raster_nodata = pygeoprocessing.get_raster_info(str(landcover_reclass))[\"nodata\"][0]\n",
    "tobler_raster_nodata = pygeoprocessing.get_raster_info(str(tobler_rescale_raster_white))[\"nodata\"][0]\n",
    "\n",
    "\n",
    "def weighted_average_op(tobler_array, lulc_cost_array, tobler_weight=0.8, lulc_cost_weight=0.2):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(tobler_array.shape, target_nodata)\n",
    "\n",
    "    # Make a masking array to ignore all nodata areas in the original data\n",
    "    valid_mask = np.full(tobler_array.shape, True)\n",
    "    valid_mask &= ~pygeoprocessing.array_equals_nodata(tobler_array, target_nodata)\n",
    "    valid_mask &= ~pygeoprocessing.array_equals_nodata(lulc_cost_array, target_nodata)\n",
    "\n",
    "    # Calculate weighted average\n",
    "    output[valid_mask] = ((tobler_array[valid_mask]*tobler_weight) + (lulc_cost_array[valid_mask]*lulc_cost_weight)) / (tobler_weight+lulc_cost_weight)\n",
    "\n",
    "    return output \n",
    "\n",
    "if cost_surface_white.exists():\n",
    "    cost_surface_white.unlink()\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(tobler_rescale_raster_white),1),(str(landcover_reclass),1)],\n",
    "    weighted_average_op,\n",
    "    str(cost_surface_white),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multicritera cost surface (Marquez-Perez 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodata values for toblers and lulc\n",
    "landcover_raster_nodata = pygeoprocessing.get_raster_info(str(landcover_reclass))[\"nodata\"][0]\n",
    "tobler_raster_nodata = pygeoprocessing.get_raster_info(str(tobler_rescale_raster_mp))[\"nodata\"][0]\n",
    "\n",
    "if cost_surface_mp.exists():\n",
    "    cost_surface_mp.unlink()\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(tobler_rescale_raster_mp),1),(str(landcover_reclass),1)],\n",
    "    weighted_average_op,\n",
    "    str(cost_surface_mp),\n",
    "    gdal.GDT_Float32,\n",
    "    target_nodata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'Advanced' Circuitscape input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buffer church locations\n",
    "church_location_gdf = gpd.read_file(church_vector_path)\n",
    "church_location_gdf.geometry = church_location_gdf.buffer(connectivity_buffer_dist_m)\n",
    "church_location_gdf.to_file(buffered_church_vector)\n",
    "\n",
    "# Rasterize buffered church locations\n",
    "pygeoprocessing.new_raster_from_base(\n",
    "    str(dem_raster_path),\n",
    "    str(circuitscape_church_raster),\n",
    "    gdal.GDT_Int8,\n",
    "    [100],\n",
    ")\n",
    "\n",
    "pygeoprocessing.rasterize(\n",
    "    str(buffered_church_vector),\n",
    "    str(circuitscape_church_raster),\n",
    "    [10],\n",
    ")\n",
    "\n",
    "# Rasterize city\n",
    "pygeoprocessing.new_raster_from_base(\n",
    "    str(dem_raster_path),\n",
    "    str(circuitscape_city_raster),\n",
    "    gdal.GDT_Int8,\n",
    "    [100],\n",
    ")\n",
    "\n",
    "pygeoprocessing.rasterize(\n",
    "    str(city_outline_vector_path),\n",
    "    str(circuitscape_city_raster),\n",
    "    [0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tiled Circuitscape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import circuitscape\n",
    "\n",
    "# gis_folder = Path(r\"C:\\Users\\lizad\\OneDrive\\Desktop\\Brown\\Dissertation\\GIS\")\n",
    "\n",
    "\n",
    "# cost_surface_raster = gis_folder / \"cost_surface.tif\"\n",
    "# workspace_path = gis_folder/ \"circuitscape\"\n",
    "# seed_ini = gis_folder / \"circuitscape/blank_circuitscape_ini.ini\"\n",
    "# # seed_ini = gis_folder / \"circuitscape/blank_circuitscape_manual.ini\"\n",
    "\n",
    "# circuitscape.execute(cost_surface_raster, seed_ini, workspace_path, keep_intermediates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing connectivity by church"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define zonal statistics function\n",
    "def connectivity_zonal_stats(connectivity_raster, vector, output_vector, join_gdf=None):\n",
    "    zonal_stats_dict = pygeoprocessing.zonal_statistics(\n",
    "        (str(connectivity_raster),1),\n",
    "        str(vector),\n",
    "    )\n",
    "        \n",
    "    zonal_stats_df = pd.DataFrame(zonal_stats_dict).T\n",
    "    zonal_stats_df.index = zonal_stats_df.index -1\n",
    "    zonal_stats_df[\"mean\"] = zonal_stats_df[\"sum\"] / zonal_stats_df[\"count\"]\n",
    "    zonal_stats_df = zonal_stats_df.rename(columns={c: f\"omnidirectional_curmap_{c}\" for c in zonal_stats_df.columns})\n",
    "\n",
    "    vector_gdf = join_gdf if join_gdf is not None else gpd.read_file(vector)\n",
    "    zonal_stats_gdf = vector_gdf.join(zonal_stats_df)\n",
    "    zonal_stats_gdf.to_file(output_vector)\n",
    "\n",
    "\n",
    "# Define full connectivity statistics function\n",
    "def connectivity_statistics(\n",
    "        church_location_gdf,\n",
    "        connectivity_raster,\n",
    "        sample_points_folder,\n",
    "\n",
    "        sample_area_buffer_m=sample_area_buffer_m,\n",
    "        sample_point_iterations=sample_point_iterations,\n",
    "\n",
    "        results_suffix = '',\n",
    "        connectivity_buffer_dist_m=connectivity_buffer_dist_m,\n",
    "        ):\n",
    "    \n",
    "    # Define results suffix\n",
    "    if results_suffix != \"\":\n",
    "        results_suffix = f\"_{results_suffix}\"\n",
    "\n",
    "    # Ensure results folder exists\n",
    "    sample_points_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # Get study area boundary for sample points analysis\n",
    "    sample_area_gdf = church_location_gdf.dissolve().envelope.buffer(sample_area_buffer_m)\n",
    "    sample_area_gdf.to_file(sample_points_folder / sample_area_vector_name.format(results_suffix))\n",
    "\n",
    "    # Buffer churches and save file\n",
    "    buffered_church_vector = sample_points_folder / buffered_church_name.format(connectivity_buffer_dist_m, results_suffix)\n",
    "    church_location_gdf.geometry = church_location_gdf.buffer(connectivity_buffer_dist_m)\n",
    "    church_location_gdf.to_file(buffered_church_vector)\n",
    "    \n",
    "    # Get connectivity raster info\n",
    "    connectivity_raster_info = pygeoprocessing.get_raster_info(str(connectivity_raster))\n",
    "\n",
    "\n",
    "    # bbox = connectivity_raster_info[\"bounding_box\"]\n",
    "    # sample_area_gdf = gpd.GeoDataFrame(pd.DataFrame(['bbox'], columns = ['geom']),\n",
    "    #          crs = connectivity_raster_info[\"projection_wkt\"],\n",
    "    #          geometry = [Polygon([[bbox[0], bbox[1]],\n",
    "    #         [bbox[2],bbox[1]],\n",
    "    #         [bbox[2],bbox[3]],\n",
    "    #         [bbox[0], bbox[3]]])\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # Select number of sample points equal to the number of churches\n",
    "    n_sample_points = len(church_location_gdf)\n",
    "\n",
    "    # Run zonal stats on churches\n",
    "    buffered_church_zonal_stats_vector = sample_points_folder / buffered_church_zonal_stat_name.format(connectivity_buffer_dist_m, results_suffix)\n",
    "    connectivity_zonal_stats(connectivity_raster, buffered_church_vector, buffered_church_zonal_stats_vector, join_gdf=church_location_gdf)\n",
    "\n",
    "    # Read in church zonal stats data and make \"church\" boolean column\n",
    "    church_stats_gdf = gpd.read_file(buffered_church_zonal_stats_vector)\n",
    "    church_stats_gdf[church_var] = True\n",
    "\n",
    "    # Extract all raster connectivity values\n",
    "    connectivity_array = pygeoprocessing.raster_to_numpy_array(str(connectivity_raster))\n",
    "    connectivity_array[connectivity_array==connectivity_raster_info[\"nodata\"]] = np.nan\n",
    "    connectivity_array = connectivity_array[~np.isnan(connectivity_array)]\n",
    "\n",
    "    # Iterate through sample point attempts, storing statistical outputs\n",
    "    zonal_outputs = [\"min\", \"max\", \"mean\"]\n",
    "    methods = [\"pointbiserial\", \"wilcoxon\"]\n",
    "    values = [\"statistic\", \"p_value\"]\n",
    "    data_dict = {\"sample_iteration\":[]}\n",
    "    data_dict_key_format = \"{}_{}_{}\"\n",
    "    data_dict.update({data_dict_key_format.format(zonal, method, value): [] for zonal, method, value in itertools.product(zonal_outputs,methods,values)})\n",
    "\n",
    "    for i in range(sample_point_iterations):\n",
    "        data_dict[\"sample_iteration\"].append(i)\n",
    "\n",
    "        # Name outputs based on the sample point number\n",
    "        sample_points_vector = sample_points_folder / sample_points_vector_name.format(results_suffix,i)\n",
    "        buffered_sample_points_vector = sample_points_folder / buffered_sample_points_vector_name.format(connectivity_buffer_dist_m, results_suffix,i)\n",
    "\n",
    "        buffered_sample_points_zonal_stats_vector = sample_points_folder / buffered_sample_points_zonal_stats_vector_name.format(connectivity_buffer_dist_m, results_suffix,i)\n",
    "        combined_stats_vector_path = sample_points_folder / combined_stats_vector_pathname.format(results_suffix,i)\n",
    "\n",
    "        # Create sample points from study area bounds\n",
    "        sampled_points = sample_area_gdf.sample_points(n_sample_points).explode()\n",
    "        sampled_points.to_file(sample_points_vector)\n",
    "\n",
    "        # Buffer sample points for zonal statistics\n",
    "        sampled_points_buffer = sampled_points.buffer(connectivity_buffer_dist_m)\n",
    "        sampled_points_buffer.to_file(buffered_sample_points_vector)\n",
    "\n",
    "        # Run zonal stats on sample points\n",
    "        connectivity_zonal_stats(\n",
    "            connectivity_raster,\n",
    "            buffered_sample_points_vector,\n",
    "            buffered_sample_points_zonal_stats_vector,\n",
    "            join_gdf=sampled_points)\n",
    "\n",
    "        # Read in sample zonal stats data and make \"church\" boolean column\n",
    "        sample_points_stats_gdf = gpd.read_file(buffered_sample_points_zonal_stats_vector)\n",
    "        sample_points_stats_gdf[church_var] = False\n",
    "\n",
    "        # Merge points into single dataset with church boolean field\n",
    "        combined_stats_gdf = pd.concat([church_stats_gdf, sample_points_stats_gdf])\n",
    "        combined_stats_gdf.to_file(combined_stats_vector_path)\n",
    "\n",
    "        # Iterate through zonal stats type (min max mean) and calculate statistics on each\n",
    "        for zonal in zonal_outputs:\n",
    "            # stats_gdf = combined_stats_gdf[[current_var,church_var]].dropna()\n",
    "            current_var = current_var_structure.format(zonal)\n",
    "\n",
    "            stats_gdf = combined_stats_gdf[[current_var,church_var]]\n",
    "\n",
    "            current_col = stats_gdf[current_var]\n",
    "            church_col = stats_gdf[church_var]\n",
    "\n",
    "            # Run correlation analysis on sample points vs churches\n",
    "            pointbiserial_statistic, pointbiserial_p_value = pointbiserialr(church_col, current_col, nan_policy=\"omit\")\n",
    "            data_dict[data_dict_key_format.format(zonal,\"pointbiserial\",\"statistic\")].append(pointbiserial_statistic)\n",
    "            data_dict[data_dict_key_format.format(zonal,\"pointbiserial\",\"p_value\")].append(pointbiserial_p_value)\n",
    "\n",
    "            # Run wilcoxon thingy\n",
    "            church_current_values = current_col[church_col]\n",
    "            sample_current_values = current_col[~church_col]\n",
    "            wilcoxon_statistic, wilcoxon_p_value = wilcoxon(church_current_values, sample_current_values, nan_policy=\"omit\")\n",
    "            data_dict[data_dict_key_format.format(zonal,\"wilcoxon\",\"statistic\")].append(wilcoxon_statistic)\n",
    "            data_dict[data_dict_key_format.format(zonal,\"wilcoxon\",\"p_value\")].append(wilcoxon_p_value)\n",
    "\n",
    "            # Create connectivity histogram\n",
    "            fig, axs = plt.subplots(1, 1, figsize =(10, 7))\n",
    "\n",
    "            # Plot raster connectivity histogram\n",
    "            plt.hist(connectivity_array, color=\"skyblue\", edgecolor=\"black\") \n",
    "            plt.xlabel(f'Connectivity ({zonal})')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Shart frequency') \n",
    "            axs.yaxis.set_ticks_position('none') \n",
    "            axs.axes.yaxis.set_ticklabels([]) \n",
    "\n",
    "            # # Plot church/sample locations as vertical lines\n",
    "            # plt.vlines(x[y],ymin=0, ymax=3500000, colors=\"red\")\n",
    "            # plt.vlines(x[~y],ymin=0, ymax=3500000, colors=\"grey\")\n",
    "\n",
    "            # Create random array of values for scatterplot visualization (y axis of churches)\n",
    "            rng = np.random.default_rng()\n",
    "            sample_random_y = rng.uniform(100000,150000,len(current_col[~church_col]))\n",
    "            church_random_y = rng.uniform(250000,350000,len(current_col[church_col]))\n",
    "\n",
    "            # Plot church/sample locations as scatterplots\n",
    "            plt.scatter(current_col[church_col],church_random_y, c=\"red\", s=100, marker=\"o\", edgecolors='black')\n",
    "            plt.scatter(current_col[~church_col],sample_random_y, c=\"grey\",s=100, marker=\"o\", edgecolors='black')\n",
    "\n",
    "            # Save figure\n",
    "            histogram_figure = sample_points_folder / histogram_name.format(results_suffix.i,zonal)\n",
    "            plt.savefig(histogram_figure, dpi=300)\n",
    "            plt.close()\n",
    "            # plt.show()\n",
    "\n",
    "    statistics_df = pd.DataFrame(data_dict)\n",
    "    statistics_df.to_csv(sample_points_folder / combined_statistics_csv_name.format(results_suffix))\n",
    "\n",
    "\n",
    "# Read in church locations\n",
    "church_location_gdf = gpd.read_file(church_vector_path)\n",
    "\n",
    "# Run connectivity statistics on all churches\n",
    "connectivity_statistics(church_location_gdf, connectivity_raster, sample_points_folder)\n",
    "\n",
    "# Run connectivity statistics on subset of churches\n",
    "church_location_gdf = church_location_gdf[church_location_gdf[church_designation_column] == church_designation_value]\n",
    "connectivity_statistics(church_location_gdf, connectivity_raster, sample_points_folder, results_suffix=church_designation_results_suffix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuitscape analysis excluding suburban churches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure iteration\n",
    "\n",
    "# Create connectivity histogram\n",
    "fig, axs = plt.subplots(1, 1, figsize =(10, 7))\n",
    "\n",
    "# Plot raster connectivity histogram \n",
    "plt.hist(connectivity_array, color=\"skyblue\", edgecolor=\"black\") \n",
    "plt.xlabel('Connectivity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Shart frequency') \n",
    "axs.yaxis.set_ticks_position('none') \n",
    "axs.axes.yaxis.set_ticklabels([]) \n",
    "\n",
    "# # Plot church/sample locations as vertical lines\n",
    "# plt.vlines(x[y],ymin=0, ymax=3500000, colors=\"red\")\n",
    "# plt.vlines(x[~y],ymin=0, ymax=3500000, colors=\"grey\")\n",
    "\n",
    "# Create random array of values for scatterplot visualization (y axis of churches)\n",
    "rng = np.random.default_rng()\n",
    "sample_random_y = rng.uniform(100000,150000,len(current_col[~church_col]))\n",
    "church_random_y = rng.uniform(250000,350000,len(current_col[church_col]))\n",
    "\n",
    "# Plot church/sample locations as scatterplots\n",
    "plt.scatter(current_col[church_col],church_random_y, c=\"red\", s=100, marker=\"o\", edgecolors='black')\n",
    "plt.scatter(current_col[~church_col],sample_random_y, c=\"grey\",s=100, marker=\"o\", edgecolors='black')\n",
    "\n",
    "# Save figure\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "pointbiserialr(church_col, current_col)\n",
    "wilcoxon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewshed analysis (DEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open church point data\n",
    "church_location_gdf = gpd.read_file(church_vector_path)\n",
    "\n",
    "# TODO Iterate through churches\n",
    "viewshed_rasters = []\n",
    "for row in church_location_gdf.itertuples():\n",
    "    church_x = row.geometry.coords[0][0]\n",
    "    church_y = row.geometry.coords[0][1]\n",
    "    output_viewshed = viewshed_results_folder / viewshed_file_name.format(getattr(row, church_name_field))\n",
    "    viewshed_rasters.append(output_viewshed)\n",
    "\n",
    "    # Open DEM dataset\n",
    "    dem_raster_ds = gdal.Open(str(dem_raster_path))\n",
    "    dem_raster_band = dem_raster_ds.GetRasterBand(1)\n",
    "\n",
    "    dem_raster_info = pygeoprocessing.get_raster_info(str(dem_raster_path))\n",
    "\n",
    "    hypotenuse_length = math.sqrt(\n",
    "        (dem_raster_info[\"raster_size\"][0]*dem_raster_info[\"pixel_size\"][0])**2 +\n",
    "        (dem_raster_info[\"raster_size\"][1]*dem_raster_info[\"pixel_size\"][1])**2\n",
    "        )\n",
    "    \n",
    "\n",
    "    # https://gdal.org/en/stable/programs/gdal_viewshed.html\n",
    "    ds = gdal.ViewshedGenerate(\n",
    "        srcBand = dem_raster_band,\n",
    "        driverName = 'GTiff',\n",
    "        targetRasterName = str(output_viewshed),\n",
    "        creationOptions = [\"INTERLEAVE=BAND\"],\n",
    "        observerX = church_x,\n",
    "        observerY = church_y,\n",
    "        observerHeight = church_height_m,\n",
    "        targetHeight = 2,\n",
    "        visibleVal = 1,\n",
    "        invisibleVal = 0,\n",
    "        outOfRangeVal = 0,\n",
    "        noDataVal = 0,\n",
    "        dfCurvCoeff = 1 - 1/7,\n",
    "        mode = gdal.GVOT_NORMAL,\n",
    "        maxDistance = viewshed_distance_m)\n",
    "\n",
    "    del ds\n",
    "\n",
    "\n",
    "# Align all viewsheds\n",
    "pygeoprocessing.align_and_resize_raster_stack(\n",
    "    [str(r) for r in viewshed_rasters],\n",
    "    [str(r.with_stem(aligned_file_name.format(r.name))) for r in viewshed_rasters],\n",
    "    [\"near\"]*len(viewshed_rasters),\n",
    "    dem_raster_info[\"pixel_size\"],\n",
    "    \"union\",\n",
    ")\n",
    "\n",
    "# Combine individual viewsheds into composite\n",
    "\n",
    "def sum_op(*arrays):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(arrays[0].shape, 0)\n",
    "\n",
    "    # Calculate weighted average\n",
    "    for r in arrays:\n",
    "        output += r\n",
    "    return output \n",
    "\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(r.with_stem(aligned_file_name.format(r.name))),1) for r in viewshed_rasters],\n",
    "    sum_op,\n",
    "    str(composite_viewshed),\n",
    "    gdal.GDT_Int8,\n",
    "    0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewshed analysis (including tree height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open church point data\n",
    "church_location_gdf = gpd.read_file(church_vector_path)\n",
    "\n",
    "# TODO Iterate through churches\n",
    "treeheight_viewshed_rasters = []\n",
    "for row in church_location_gdf.itertuples():\n",
    "    church_x = row.geometry.coords[0][0]\n",
    "    church_y = row.geometry.coords[0][1]\n",
    "    output_viewshed = viewshed_treeheight_results_folder / treeheight_viewshed_file_name.format(getattr(row, church_name_field))\n",
    "    treeheight_viewshed_rasters.append(output_viewshed)\n",
    "\n",
    "    # Open DEM dataset\n",
    "    treeheight_raster_ds = gdal.Open(str(dem_treeheight_raster_path))\n",
    "    treeheight_raster_band = treeheight_raster_ds.GetRasterBand(1)\n",
    "\n",
    "    treeheight_raster_info = pygeoprocessing.get_raster_info(str(dem_treeheight_raster_path))\n",
    "\n",
    "    hypotenuse_length = math.sqrt(\n",
    "        (treeheight_raster_info[\"raster_size\"][0]*treeheight_raster_info[\"pixel_size\"][0])**2 +\n",
    "        (treeheight_raster_info[\"raster_size\"][1]*treeheight_raster_info[\"pixel_size\"][1])**2\n",
    "        )\n",
    "    \n",
    "\n",
    "    # https://gdal.org/en/stable/programs/gdal_viewshed.html\n",
    "    ds = gdal.ViewshedGenerate(\n",
    "        srcBand = treeheight_raster_band,\n",
    "        driverName = 'GTiff',\n",
    "        targetRasterName = str(output_viewshed),\n",
    "        creationOptions = [\"INTERLEAVE=BAND\"],\n",
    "        observerX = church_x,\n",
    "        observerY = church_y,\n",
    "        observerHeight = church_height_m,\n",
    "        targetHeight = 2,\n",
    "        visibleVal = 1,\n",
    "        invisibleVal = 0,\n",
    "        outOfRangeVal = 0,\n",
    "        noDataVal = 0,\n",
    "        dfCurvCoeff = 1 - 1/7,\n",
    "        mode = gdal.GVOT_NORMAL,\n",
    "        maxDistance = viewshed_distance_m)\n",
    "    \n",
    "    del ds\n",
    "\n",
    "aligning_rasters = [treeheight_raster_path] + treeheight_viewshed_rasters\n",
    "pygeoprocessing.align_and_resize_raster_stack(\n",
    "    [str(r) for r in aligning_rasters],\n",
    "    [str(r.with_stem(aligned_file_name.format(r.name))) for r in aligning_rasters],\n",
    "    [\"near\"]*len(aligning_rasters),\n",
    "    treeheight_raster_info[\"pixel_size\"],\n",
    "    \"union\",\n",
    ")\n",
    "\n",
    "\n",
    "# Combine individual viewsheds into composite\n",
    "\n",
    "def sum_minus_trees_op(tree_canopy_array, *arrays, tree_height_cutoff=1.6):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(arrays[0].shape, 0)\n",
    "\n",
    "    # Calculate sum\n",
    "    for r in arrays:\n",
    "        output += r\n",
    "\n",
    "    # Filter by areas with tree canopy less than the threshold\n",
    "    tree_canopy_filter = tree_canopy_array <= tree_height_cutoff\n",
    "\n",
    "    output *= tree_canopy_filter\n",
    "\n",
    "    return output \n",
    "\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(r.with_stem(aligned_file_name.format(r.name))),1) for r in aligning_rasters],\n",
    "    sum_minus_trees_op,\n",
    "    str(composite_treeheight_viewshed),\n",
    "    gdal.GDT_Int8,\n",
    "    0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative viewshed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in bounding box for sample points\n",
    "viewshed_sample_area_gdf = gpd.read_file(viewshed_sample_area)\n",
    "\n",
    "# Create sample points in study area bounds\n",
    "viewshed_sample_points = viewshed_sample_area_gdf.sample_points(viewshed_sample_point_iterations).explode()\n",
    "viewshed_sample_points.to_file(viewshed_sample_points_vector_name)\n",
    "\n",
    "viewshed_sample_points_gdf = gpd.read_file(viewshed_sample_points_vector_name)\n",
    "\n",
    "random_viewshed_rasters = []\n",
    "for i, row in enumerate(viewshed_sample_points_gdf.itertuples()):\n",
    "    point_x = row.geometry.coords[0][0]\n",
    "    point_y = row.geometry.coords[0][1]\n",
    "    output_viewshed = cumulative_viewshed_analysis_folder / viewshed_file_name.format(i)\n",
    "    random_viewshed_rasters.append(output_viewshed)\n",
    "\n",
    "     # Open DEM dataset\n",
    "    treeheight_raster_ds = gdal.Open(str(dem_treeheight_raster_path))\n",
    "    treeheight_raster_band = treeheight_raster_ds.GetRasterBand(1)\n",
    "    treeheight_raster_info = pygeoprocessing.get_raster_info(str(dem_treeheight_raster_path))\n",
    "\n",
    "    #Run viewshed analysis on each sample point\n",
    "       # https://gdal.org/en/stable/programs/gdal_viewshed.html\n",
    "    ds = gdal.ViewshedGenerate(\n",
    "        srcBand = treeheight_raster_band,\n",
    "        driverName = 'GTiff',\n",
    "        targetRasterName = str(output_viewshed),\n",
    "        creationOptions = [\"INTERLEAVE=BAND\"],\n",
    "        observerX = point_x,\n",
    "        observerY = point_y,\n",
    "        observerHeight = 1.6,\n",
    "        targetHeight = 2,\n",
    "        visibleVal = 1,\n",
    "        invisibleVal = 0,\n",
    "        outOfRangeVal = 0,\n",
    "        noDataVal = 0,\n",
    "        dfCurvCoeff = 1 - 1/7,\n",
    "        mode = gdal.GVOT_NORMAL,\n",
    "        maxDistance = viewshed_distance_m)\n",
    "    \n",
    "    del ds\n",
    "\n",
    "# Combine individual viewsheds into composite\n",
    "def sum_minus_trees_op(tree_canopy_array, *arrays, tree_height_cutoff=1.6):\n",
    "    # Make an array of the same shape full of nodata\n",
    "    output = np.full(arrays[0].shape, 0)\n",
    "\n",
    "    # Calculate sum\n",
    "    for r in arrays:\n",
    "        output += r\n",
    "\n",
    "    # Filter by areas with tree canopy less than the threshold\n",
    "    tree_canopy_filter = tree_canopy_array <= tree_height_cutoff\n",
    "\n",
    "    output *= tree_canopy_filter\n",
    "\n",
    "    return output \n",
    "\n",
    "aligning_rasters = [treeheight_raster_path] + random_viewshed_rasters\n",
    "pygeoprocessing.align_and_resize_raster_stack(\n",
    "    [str(r) for r in aligning_rasters],\n",
    "    [str(r.with_stem(aligned_file_name.format(r.name))) for r in aligning_rasters],\n",
    "    [\"near\"]*len(aligning_rasters),\n",
    "    treeheight_raster_info[\"pixel_size\"],\n",
    "    \"union\",\n",
    ")\n",
    "\n",
    "pygeoprocessing.raster_calculator(\n",
    "    [(str(r.with_stem(aligned_file_name.format(r.name))),1) for r in aligning_rasters],\n",
    "    sum_minus_trees_op,\n",
    "    str(random_composite_viewshed),\n",
    "    gdal.GDT_Int8,\n",
    "    0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewsheds and LCP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open church point data\n",
    "church_location_gdf = gpd.read_file(church_vector_path, engine=\"pyogrio\", fid_as_index=True)\n",
    "multicriteria_lcps_gdf = gpd.read_file(multicriteria_lcps_vector_path)\n",
    "slope_lcps_gdf = gpd.read_file(slope_lcps_vector_path)\n",
    "\n",
    "# Iterate through churches\n",
    "treeheight_viewshed_rasters = []\n",
    "for row in church_location_gdf.itertuples():\n",
    "    output_viewshed = viewshed_treeheight_results_folder / treeheight_viewshed_file_name.format(getattr(row, church_name_field))\n",
    "    if not output_viewshed.exists():\n",
    "        raise ValueError(\"No viewshed for this church has been found\")\n",
    "    multicriteria_lcp = multicriteria_lcps_gdf[multicriteria_lcps_gdf[\"end point id\"]==row.Index]\n",
    "    slope_lcp = slope_lcps_gdf[slope_lcps_gdf[\"end point id\"]==row.Index]\n",
    "    output_viewshed_ds = gdal.Open(str(output_viewshed))\n",
    "    output_viewshed_band = output_viewshed_ds.GetRasterBand(1)\n",
    "    viewshed_crs = output_viewshed_ds.GetSpatialRef()\n",
    "    output_viewshed_polygon = gis_folder / f\"viewsheds/LCP_analysis/church_polygonized_viewshed_{getattr(row, church_name_field)}.gpkg\"\n",
    "    if output_viewshed_polygon.exists():\n",
    "        output_viewshed_polygon.unlink()\n",
    "    \n",
    "    # Create output vector layer using GDAL, which is very dumb\n",
    "    drv = ogr.GetDriverByName(\"GPKG\")\n",
    "    viewshed_field = 'viewshed'\n",
    "    ds = drv.CreateDataSource(str(output_viewshed_polygon))\n",
    "    layer = ds.CreateLayer(output_viewshed_polygon.stem, srs = viewshed_crs)\n",
    "    field_defn = ogr.FieldDefn(viewshed_field, ogr.OFTInteger)\n",
    "    layer.CreateField(field_defn)\n",
    "    gdal.Polygonize(output_viewshed_band, None, layer, 0, [], callback=None)\n",
    "    # Subset to no viewshed (i.e. value of 0) and delete\n",
    "    layer.SetAttributeFilter(f\"{viewshed_field} = 0\")\n",
    "    for feature in layer:\n",
    "        fid = feature.GetFID()\n",
    "        layer.DeleteFeature(int(fid))\n",
    "\n",
    "    del ds\n",
    "    del layer\n",
    "\n",
    "    # Calculate intersection between viewshed and LCP\n",
    "    output_viewshed_gdf = gpd.read_file(output_viewshed_polygon)\n",
    "    output_viewshed_gdf = output_viewshed_gdf.dissolve()\n",
    "\n",
    "    test = slope_lcp.overlay(output_viewshed_gdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.020077582963266657)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate intersection between viewshed and LCP\n",
    "output_viewshed_gdf = gpd.read_file(output_viewshed_polygon)\n",
    "output_viewshed_gdf = output_viewshed_gdf.dissolve()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "slope_lcp = slope_lcps_gdf[slope_lcps_gdf[\"end point id\"]==19]\n",
    "\n",
    "\n",
    "test = slope_lcp.overlay(output_viewshed_gdf)\n",
    "\n",
    "# output_viewshed_polygon\n",
    "# slope_lcp\n",
    "test.length\n",
    "slope_lcp.length.values[0]\n",
    "\n",
    "test.length.values[0] / slope_lcp.length.values[0]\n",
    "\n",
    "\n",
    "# # Plotting for testing\n",
    "# fig, ax = plt.subplots()\n",
    "# output_viewshed_gdf.plot(ax=ax)\n",
    "# slope_lcps_gdf.plot(ax=ax)\n",
    "# slope_lcp.plot(ax=ax, color=\"red\")\n",
    "# test.plot(ax=ax, color=\"red\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb tree data download CLI commands for dumb trees\n",
    "\n",
    "### THIS IS ALL IN THE TERMINAL. SCARY\n",
    "\n",
    "### Go to folder where you will download things\n",
    "# cd FOLDER_YOU_WILL_DOWNLOAD_TO\n",
    "\n",
    "### Scan AWS resource folder (SKIP IF YOU WANT)\n",
    "# aws s3 ls --no-sign-request s3://dataforgood-fb-data/forests/v1/alsgedi_global_v6_float/ --human-readable\n",
    "\n",
    "### Download tile file for IDing which tiles to actually download (SKIP SINCE WE ALREADY DOWNLOADED IT)\n",
    "# aws s3 cp --no-sign-request s3://dataforgood-fb-data/forests/v1/alsgedi_global_v6_float/tiles.geojson tiles.geojson\n",
    "\n",
    "### Download an individual tile \n",
    "# aws s3 cp --no-sign-request s3://dataforgood-fb-data/forests/v1/alsgedi_global_v6_float/chm/[NUMBER OF TILE].tif tree_height_[NUMBER OF TILE].tif\n",
    "\n",
    "# tiles\n",
    "aphrodisias = [122101202, 122101203, 122101220, 122101221]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nci-dev-invest314-current",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
